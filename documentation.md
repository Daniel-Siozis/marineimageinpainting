# DeepFish

Muriz Ganic (it78iwat/23215741), Daniel Siozis (ha04giji/23171488), Berkan Türkel (yb57ogox/22833043), Samuel Arapoglu (os78odaz/23198175) 

## 1 Introduction

## 2 Related Work

## 3 Methodology
### 3.1 General Methodology
- Downloaded DeepFish dataset 
- Cloned DeepFish GitHub repository
- Researched other similar projects
- Build a model architecture
- Wrote program to create broken images
- Trained model
- Deployed streamlit app

### 3.2 Data Understanding and Preparation
We used the JCU DeepFish dataset for our project.
The dataset originally consists of approximately 40 thousand images collected underwater from 20 habitats in the marine-environments of tropical Australia. The dataset contained classification, point-level and segmentation labels. Videos for DeepFish were collected for 20 habitats from remote coastal marine environments of tropical Australia. These videos were acquired using cameras mounted on metal frames, deployed over the side of a vessel to acquire video footage underwater. The cameras were lowered to the seabed and left to record the natural fish community, while the vessel maintained a distance of 100 m. The depth and the map coordinates of the cameras were collected using an acoustic depth sounder and a GPS, respectively. Video recording was carried out during daylight hours and in relatively low turbidity periods. The video clips were captured in full HD resolution (1920 × 1080 pixels) from a digital camera. In total, the number of video frames taken is 39,766.
For our project, we also wrote a program that allowed us to create "broken" images from the existing images. In our case pictures with randomly arranged black boxes of different sizes. So pictures with a black mask. We also deleted …
### 3.3 Modeling and Evaluation

## 4 Results

## 5 Discussion

## 6 Conclusion

